{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ace1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random as rdm\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from Gridworld import Gridworld\n",
    "from matplotlib import pylab as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variabeln \n",
    "Inputlayer = 64\n",
    "hiddenlayer1 = 150\n",
    "hiddenlayer2 = 100\n",
    "outputlayer = 4\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "Lernrate = 1e-3\n",
    "epochen = 5000 # gibt an wie lange der algorithmus trainieren soll.\n",
    "testepochen = 3\n",
    "Gedächnisgrösse = 1000\n",
    "stapelgrösse = 200 # gibt an wiehoch die menge der zum lernen verwendeten Daten ist bevor der Algorithmus aktualisiert wird.\n",
    "max_züge = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8451379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionarys\n",
    "aktionsraum = {0:'u', 1:'d', 2: 'l', 3: 'r',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# richte neuronales Netz ein.\n",
    "model = torch.nn.Sequential(torch.nn.Linear(Inputlayer, hiddenlayer1), torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(hiddenlayer1, hiddenlayer2), torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(hiddenlayer2, outputlayer))\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Lernrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7747f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "erinnerungen = deque(maxlen=Gedächnisgrösse) # Erzeugt das Gedächniss \n",
    "h = 0\n",
    "\n",
    "for i in range(epochen):\n",
    "    # zeigt in welcher Episode wir uns befinden.\n",
    "    print(i+1)\n",
    "    clear_output(wait=True)\n",
    "    # initialisiere Spiel\n",
    "    game = Gridworld(size=4, mode='random')\n",
    "    # dem Zustand wird ein kleiner Zufallswert hinzugefügt damit kein Wert genau 0 ist.\n",
    "    # da viele Funktionen für 0 nicht deffiniert sind kann das ansonsten zu Fehlern führen.\n",
    "    zustand1 = torch.from_numpy(game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0).float() \n",
    "    aktiv = True\n",
    "    zug = 0\n",
    "    \n",
    "    # wird ausgeführ solange das Spiel aktiv ist.\n",
    "    while(aktiv == True):\n",
    "        zug += 1\n",
    "        qwert = model(zustand1) \n",
    "        q_wert = qwert.data.numpy()\n",
    "        # mit der Wahrscheinlichkeit Epsilon wird eine zufällige Aktion gewählt.\n",
    "        if (rdm.random() < epsilon): \n",
    "            aktion = np.random.randint(0,4)\n",
    "        else:\n",
    "            aktion = np.argmax(q_wert)\n",
    "        \n",
    "        # aktion wird im spiel angewendet\n",
    "        aktionstring = aktionsraum[aktion] \n",
    "        game.makeMove(aktionstring)\n",
    "        # generiert neuen zustand\n",
    "        zustand_2 = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0\n",
    "        zustand2 = torch.from_numpy(zustand_2).float()\n",
    "        # speichert erfahrungen\n",
    "        belohnung = game.reward()\n",
    "        done = True if belohnung > 0 else False # gibt an ob wir für dieses Feld positiv belohnt wurden.\n",
    "        erfahrung = (zustand1, aktion, belohnung, zustand2, done)\n",
    "        erinnerungen.append(erfahrung)\n",
    "        zustand1 = zustand2\n",
    "        \n",
    "        # der Q learning Algorithmus wird trainiert wenn genug erinnerungen gesammelt wurden \n",
    "        if len(erinnerungen) > stapelgrösse:\n",
    "            # eine stichprobe wird zum trainieren des Algorithmus bereit gemacht. \n",
    "            stichprobe = rdm.sample(erinnerungen, stapelgrösse)\n",
    "            zustand1_probe = torch.cat([s1 for (s1,a,r,s2,d) in stichprobe])\n",
    "            zustand2_probe = torch.cat([s2 for (s1,a,r,s2,d) in stichprobe])\n",
    "            belohnung_probe = torch.Tensor([r for (s1,a,r,s2,d) in stichprobe])\n",
    "            aktion_probe = torch.Tensor([a for (s1,a,r,s2,d) in stichprobe])\n",
    "            done_probe = torch.Tensor([d for (s1,a,r,s2,d) in stichprobe])\n",
    "            # berechnet Q-Wert für die Stichprobe \n",
    "            Q_t1 = model(zustand1_probe)\n",
    "            with torch.no_grad():\n",
    "                Q_t2 = model(zustand2_probe)\n",
    "            # Berechnet die Werte die das Netzwerk lernen soll\n",
    "            Y = belohnung_probe + gamma * ((1 - done_probe) * torch.max(Q_t2,dim=1)[0])\n",
    "            X = Q_t1.gather(dim=1,index=aktion_probe.long().unsqueeze(dim=1)).squeeze()\n",
    "            loss = loss_fn(X, Y.detach())\n",
    "            print(i, loss.item())\n",
    "            clear_output(wait=True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "                \n",
    "        if belohnung != -1 or zug >= max_züge: \n",
    "            aktiv = False\n",
    "            zug = 0\n",
    "losses = np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit dieser Funktion können wir den trainierten Algorithmus testen und sehen wie er Spielt.\n",
    "for i in range(testepochen):\n",
    "    test_game = Gridworld(mode=\"random\")\n",
    "    zustand = torch.from_numpy(test_game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0).float() \n",
    "    aktiv = True\n",
    "    print(\"neues Spiel\")\n",
    "    zug = 1\n",
    "    while(aktiv == True):\n",
    "        qwert = model(zustand) \n",
    "        q_wert = qwert.data.numpy()\n",
    "        aktion = np.argmax(q_wert)\n",
    "        aktionstring = aktionsraum[aktion] \n",
    "        test_game.makeMove(aktionstring)\n",
    "        print(f\" Zug \", zug ,\"\")\n",
    "        zug += 1\n",
    "        print(test_game.display())\n",
    "        belohnung = test_game.reward()\n",
    "        zustand = torch.from_numpy(test_game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0).float()\n",
    "        if belohnung != -1 or zug>20:\n",
    "            aktiv = False\n",
    "            if belohnung >= 0:\n",
    "                print(\"Gewonnen!\")\n",
    "            else:\n",
    "                print(\"verloren :-(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6fea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
